# wikipedia-crawler
This is a program to crawl entire Wikipedia using breath-first method and extract information from all the pages.

## Summary
This is a Python program to crawl wikipedia pages and extract textual inforation from the pages. It crawls starting with specified pages in the begining of the script. For each specified page, the algorithm find ang get the information of 200 related pages.

## Compatability
This program is written in Python 2.7 and can run on any version of Python (2.x). It is a download-and-run program with couple of changes according to user's requirements.

## Status
This is first stable version of the program which is ready-to-run, but still under development.

## Disclaimer
This program lets you crawl the pages of wikipedia for information. It downloads then entire page of wikipedia that it is currently crawling. If this program is let to run for a long period of time, it can crawl the entire online database of wikipedia, which is highly discouraged!!!

Please use this program only for educational purpose by reviewing the content (in form of text, images, graphics or any other form) copyright and its licence. Also, crawling pages in a sequential manner puts a lot of pressure on Wikipedia servers. Please follow the Wikipedia robot.txt guidelines to understand thee restrictions on number of requests per second that can be made by an external robot. According to wikipedia friendly, low speed bots are allowed to crawl!
